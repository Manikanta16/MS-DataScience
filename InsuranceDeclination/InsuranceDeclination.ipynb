{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = pd.read_csv('HW4-data.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flood</th>\n",
       "      <th>MinorityPop</th>\n",
       "      <th>FireReport</th>\n",
       "      <th>CrimeRate</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Income</th>\n",
       "      <th>Declination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>82.31</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>42.6</td>\n",
       "      <td>214.80</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>78.5</td>\n",
       "      <td>111.04</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>106.94</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>21.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.8</td>\n",
       "      <td>96.31</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>43.1</td>\n",
       "      <td>29.1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>82.7</td>\n",
       "      <td>79.95</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>137.22</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>162.50</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>136.86</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.8</td>\n",
       "      <td>124.05</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>51.2</td>\n",
       "      <td>121.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>85.1</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>127.65</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>84.2</td>\n",
       "      <td>110.84</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>15.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>89.8</td>\n",
       "      <td>105.10</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>16.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>97.84</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>94.4</td>\n",
       "      <td>18.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>72.9</td>\n",
       "      <td>73.42</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>86.2</td>\n",
       "      <td>36.2</td>\n",
       "      <td>41.0</td>\n",
       "      <td>63.1</td>\n",
       "      <td>65.65</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>50.2</td>\n",
       "      <td>39.7</td>\n",
       "      <td>147.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.59</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>74.2</td>\n",
       "      <td>18.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>78.3</td>\n",
       "      <td>80.14</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>55.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.77</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>12.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>82.12</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>117.44</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>22.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>44.0</td>\n",
       "      <td>76.5</td>\n",
       "      <td>93.23</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>99.48</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>106.56</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>24.5</td>\n",
       "      <td>8.6</td>\n",
       "      <td>53.0</td>\n",
       "      <td>81.4</td>\n",
       "      <td>97.30</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.5</td>\n",
       "      <td>112.30</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>46.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.1</td>\n",
       "      <td>83.30</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>99.7</td>\n",
       "      <td>21.6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>55.83</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>73.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>85.64</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>121.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>61.8</td>\n",
       "      <td>118.76</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>48.8</td>\n",
       "      <td>28.6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>97.42</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>98.9</td>\n",
       "      <td>17.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>68.6</td>\n",
       "      <td>75.20</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>90.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>73.4</td>\n",
       "      <td>73.88</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>238.42</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>71.2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>110.40</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>94.1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>103.32</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>66.1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>109.08</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>36.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>111.56</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>133.23</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>42.5</td>\n",
       "      <td>10.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.8</td>\n",
       "      <td>129.60</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>35.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>57.8</td>\n",
       "      <td>112.60</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>47.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>100.80</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>49.2</td>\n",
       "      <td>114.28</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>27.0</td>\n",
       "      <td>46.6</td>\n",
       "      <td>137.31</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>270.20</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>48.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>29.3</td>\n",
       "      <td>62.6</td>\n",
       "      <td>85.20</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>73.50</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Flood  MinorityPop  FireReport  CrimeRate  HouseAge  Income  Declination\n",
       "0       2         54.0        34.1       68.0      52.6   82.31         0.30\n",
       "1       2          4.9        11.0       75.0      42.6  214.80         0.02\n",
       "2       1          7.1         6.9       18.0      78.5  111.04         0.02\n",
       "3       3          5.3         7.3       31.0      90.1  106.94         0.40\n",
       "4       1         21.5        15.1       25.0      89.8   96.31         1.10\n",
       "5       1         43.1        29.1       34.0      82.7   79.95         1.90\n",
       "6       1          1.1         2.2       14.0      40.2  137.22         0.02\n",
       "7       3          1.0         5.7       11.0      27.9  162.50         0.02\n",
       "8       2          1.7         2.0       11.0       7.7  136.86         0.02\n",
       "9       1          1.6         2.5       22.0      63.8  124.05         0.02\n",
       "10      2          1.5         3.0       17.0      51.2  121.98         0.02\n",
       "11      3          1.8         5.4       27.0      85.1  116.00         0.02\n",
       "12      3          1.0         2.2        9.0      44.4  127.65         0.02\n",
       "13      3          2.5         7.2       29.0      84.2  110.84         0.20\n",
       "14      2         13.4        15.1       30.0      89.8  105.10         0.80\n",
       "15      1         59.8        16.5       40.0      72.7   97.84         0.80\n",
       "16      1         94.4        18.4       32.0      72.9   73.42         1.80\n",
       "17      3         86.2        36.2       41.0      63.1   65.65         1.80\n",
       "18      1         50.2        39.7      147.0      83.0   74.59         0.90\n",
       "19      3         74.2        18.5       22.0      78.3   80.14         1.90\n",
       "20      1         55.5        23.3       29.0      79.0   81.77         1.50\n",
       "21      2         62.3        12.2       46.0      48.0   82.12         0.60\n",
       "22      2         10.0         6.2       29.0      60.4  117.44         0.02\n",
       "23      3         22.2         9.5       44.0      76.5   93.23         0.10\n",
       "24      2         19.6        10.5       36.0      73.5   99.48         1.20\n",
       "25      3         17.3         7.7       37.0      66.9  106.56         0.50\n",
       "26      2         24.5         8.6       53.0      81.4   97.30         0.70\n",
       "27      3          4.4         5.6       23.0      71.5  112.30         0.30\n",
       "28      1         46.2        21.8        4.0      73.1   83.30         1.30\n",
       "29      1         99.7        21.6       31.0      65.0   55.83         0.90\n",
       "30      1         73.5         9.0       39.0      75.4   85.64         0.40\n",
       "31      3         10.7         3.6       15.0      20.8  121.02         0.02\n",
       "32      2          1.5         5.0       32.0      61.8  118.76         0.02\n",
       "33      1         48.8        28.6       27.0      78.1   97.42         1.40\n",
       "34      1         98.9        17.4       32.0      68.6   75.20         2.20\n",
       "35      1         90.6        11.3       34.0      73.4   73.88         0.80\n",
       "36      2          1.4         3.4       17.0       2.0  238.42         0.02\n",
       "37      2         71.2        11.9       46.0      57.0  110.40         0.90\n",
       "38      2         94.1        10.5       42.0      55.9  103.32         0.90\n",
       "39      3         66.1        10.7       43.0      67.5  109.08         0.40\n",
       "40      1         36.4        10.8       34.0      58.0  111.56         0.90\n",
       "41      2          1.0         4.8       19.0      15.2  133.23         0.02\n",
       "42      3         42.5        10.4       25.0      40.8  129.60         0.50\n",
       "43      3         35.1        15.6       28.0      57.8  112.60         1.00\n",
       "44      3         47.4         7.0        3.0      11.4  100.80         0.20\n",
       "45      2         34.0         7.1       23.0      49.2  114.28         0.30\n",
       "46      1          3.1         4.9       27.0      46.6  137.31         0.02\n",
       "47      2         23.7         1.5       18.0      22.0  270.20         0.07\n",
       "48      3         48.2         3.6       29.3      62.6   85.20         0.75\n",
       "49      1         18.0         7.3       31.2      18.2   73.50         1.20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = houses.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['intercept'] = [1]*len(X)\n",
    "X[['Flood_2','Flood_3']] = pd.get_dummies(houses['Flood'],prefix='Flood',drop_first=True)\n",
    "X.drop(labels=['Flood'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = X['Declination']/100\n",
    "X.drop(labels=['Declination'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MinorityPop', 'FireReport', 'CrimeRate', 'HouseAge', 'Income',\n",
       "       'intercept', 'Flood_2', 'Flood_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_scaled = X[['intercept', 'Flood_2', 'Flood_3']]\n",
    "list(X_scaled.columns).extend(['MinorityPop', 'FireReport', 'CrimeRate', 'HouseAge', 'Income'])\n",
    "X_scaled[['MinorityPop', 'FireReport', 'CrimeRate', 'HouseAge', 'Income']] = pd.DataFrame(data = ss.fit_transform(X[['MinorityPop', 'FireReport', 'CrimeRate', 'HouseAge', 'Income']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_train, X_scaled_test, y_scaled_train, y_scaled_test = train_test_split(X_scaled,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c17546160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(houses['Declination'],rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c176d17b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(houses['Income'],y)\n",
    "plt.plot(houses['Income'],[0]*len(y),color='orange',label='0 line')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Declination')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flood         -0.309176\n",
       "MinorityPop    0.692207\n",
       "FireReport     0.679315\n",
       "CrimeRate      0.153963\n",
       "HouseAge       0.432281\n",
       "Income        -0.575247\n",
       "Declination    1.000000\n",
       "Name: Declination, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses.corr()['Declination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c17711e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.heatmap(houses.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for individual Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with:  MinorityPop\n",
      "Mean_absolute_error:  0.00395153713733702\n",
      "R_squared:  0.37411789063499956\n",
      "\n",
      "Model with:  FireReport\n",
      "Mean_absolute_error:  0.004047663342427103\n",
      "R_squared:  0.18272211876945355\n",
      "\n",
      "Model with:  CrimeRate\n",
      "Mean_absolute_error:  0.004991925774081022\n",
      "R_squared:  0.01159529400270054\n",
      "\n",
      "Model with:  HouseAge\n",
      "Mean_absolute_error:  0.004331743339509267\n",
      "R_squared:  0.18774944877910882\n",
      "\n",
      "Model with:  Income\n",
      "Mean_absolute_error:  0.00347838953972599\n",
      "R_squared:  0.4508285572172108\n",
      "\n",
      "Model with Flood_2 and Flood_3\n",
      "Mean_absolute_error:  0.004361596782322592\n",
      "R_squared:  0.1836330781642127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for col in X.drop(labels=['Flood_2','Flood_3','intercept'],axis=1).columns:\n",
    "    print('Model with: ',col)\n",
    "    model = sm.GLM(y_train,X_train[['intercept',col]],family=sm.families.Binomial(sm.families.links.logit)).fit()\n",
    "    print('Mean_absolute_error: ',mean_absolute_error(y_train,model.predict(X_train[['intercept',col]])))\n",
    "    print('R_squared: ',r2_score(y_train,model.predict(X_train[['intercept',col]])))\n",
    "    print()\n",
    "\n",
    "print('Model with Flood_2 and Flood_3')    \n",
    "model = sm.GLM(y_train,X_train[['intercept','Flood_2','Flood_3']],family=sm.families.Binomial(sm.families.links.logit)).fit()\n",
    "print('Mean_absolute_error: ',mean_absolute_error(y_train,model.predict(X_train[['intercept','Flood_2','Flood_3']])))\n",
    "print('R_squared: ',r2_score(y_train,model.predict(X_train[['intercept','Flood_2','Flood_3']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06449464705587504"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,model.predict(X_test[['intercept','Flood_2','Flood_3']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Declination</td>   <th>  R-squared:         </th> <td>   0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   10.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 01 Dec 2021</td> <th>  Prob (F-statistic):</th> <td>3.09e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:13:26</td>     <th>  Log-Likelihood:    </th> <td>  190.73</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    45</td>      <th>  AIC:               </th> <td>  -365.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    37</td>      <th>  BIC:               </th> <td>  -351.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MinorityPop</th> <td> 7.382e-05</td> <td> 2.34e-05</td> <td>    3.155</td> <td> 0.003</td> <td> 2.64e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FireReport</th>  <td>    0.0003</td> <td>    0.000</td> <td>    3.219</td> <td> 0.003</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CrimeRate</th>   <td>-8.657e-05</td> <td> 3.36e-05</td> <td>   -2.573</td> <td> 0.014</td> <td>   -0.000</td> <td>-1.84e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HouseAge</th>    <td> 3.345e-05</td> <td> 3.29e-05</td> <td>    1.016</td> <td> 0.316</td> <td>-3.33e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Income</th>      <td>-1.531e-05</td> <td> 2.05e-05</td> <td>   -0.745</td> <td> 0.461</td> <td>-5.69e-05</td> <td> 2.63e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>   <td>    0.0028</td> <td>    0.004</td> <td>    0.693</td> <td> 0.493</td> <td>   -0.005</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Flood_2</th>     <td>   -0.0005</td> <td>    0.002</td> <td>   -0.298</td> <td> 0.767</td> <td>   -0.004</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Flood_3</th>     <td>   -0.0007</td> <td>    0.002</td> <td>   -0.422</td> <td> 0.676</td> <td>   -0.004</td> <td>    0.003</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.249</td> <th>  Durbin-Watson:     </th> <td>   2.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.325</td> <th>  Jarque-Bera (JB):  </th> <td>   1.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.212</td> <th>  Prob(JB):          </th> <td>   0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.719</td> <th>  Cond. No.          </th> <td>    976.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            Declination   R-squared:                       0.667\n",
       "Model:                            OLS   Adj. R-squared:                  0.604\n",
       "Method:                 Least Squares   F-statistic:                     10.60\n",
       "Date:                Wed, 01 Dec 2021   Prob (F-statistic):           3.09e-07\n",
       "Time:                        11:13:26   Log-Likelihood:                 190.73\n",
       "No. Observations:                  45   AIC:                            -365.5\n",
       "Df Residuals:                      37   BIC:                            -351.0\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "MinorityPop  7.382e-05   2.34e-05      3.155      0.003    2.64e-05       0.000\n",
       "FireReport      0.0003      0.000      3.219      0.003       0.000       0.001\n",
       "CrimeRate   -8.657e-05   3.36e-05     -2.573      0.014      -0.000   -1.84e-05\n",
       "HouseAge     3.345e-05   3.29e-05      1.016      0.316   -3.33e-05       0.000\n",
       "Income      -1.531e-05   2.05e-05     -0.745      0.461   -5.69e-05    2.63e-05\n",
       "intercept       0.0028      0.004      0.693      0.493      -0.005       0.011\n",
       "Flood_2        -0.0005      0.002     -0.298      0.767      -0.004       0.003\n",
       "Flood_3        -0.0007      0.002     -0.422      0.676      -0.004       0.003\n",
       "==============================================================================\n",
       "Omnibus:                        2.249   Durbin-Watson:                   2.113\n",
       "Prob(Omnibus):                  0.325   Jarque-Bera (JB):                1.306\n",
       "Skew:                           0.212   Prob(JB):                        0.520\n",
       "Kurtosis:                       3.719   Cond. No.                         976.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32    0.001549\n",
       "12    0.001679\n",
       "6     0.001624\n",
       "24    0.005047\n",
       "11    0.002779\n",
       "22    0.002807\n",
       "7     0.001579\n",
       "8     0.000302\n",
       "5     0.014197\n",
       "1    -0.002048\n",
       "26    0.003605\n",
       "16    0.014387\n",
       "30    0.009023\n",
       "34    0.014217\n",
       "38    0.009379\n",
       "21    0.007309\n",
       "42    0.005922\n",
       "14    0.007090\n",
       "31    0.001651\n",
       "27    0.002985\n",
       "37    0.007735\n",
       "4     0.008734\n",
       "25    0.003355\n",
       "41    0.000791\n",
       "49    0.003310\n",
       "15    0.010131\n",
       "19    0.013215\n",
       "0     0.012193\n",
       "2     0.004958\n",
       "36   -0.001521\n",
       "48    0.005128\n",
       "18    0.008540\n",
       "45    0.005070\n",
       "28    0.014234\n",
       "47   -0.000408\n",
       "3     0.003626\n",
       "20    0.013474\n",
       "43    0.007667\n",
       "13    0.003302\n",
       "33    0.014637\n",
       "29    0.015929\n",
       "35    0.011592\n",
       "39    0.007412\n",
       "9     0.002058\n",
       "46    0.001753\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_pred = ols.predict(X_train)\n",
    "ols_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinorityPop</th>\n",
       "      <th>FireReport</th>\n",
       "      <th>CrimeRate</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Income</th>\n",
       "      <th>intercept</th>\n",
       "      <th>Flood_2</th>\n",
       "      <th>Flood_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>61.8</td>\n",
       "      <td>118.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>127.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>137.22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>99.48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>85.1</td>\n",
       "      <td>116.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>117.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>162.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>136.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43.1</td>\n",
       "      <td>29.1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>82.7</td>\n",
       "      <td>79.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>42.6</td>\n",
       "      <td>214.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24.5</td>\n",
       "      <td>8.6</td>\n",
       "      <td>53.0</td>\n",
       "      <td>81.4</td>\n",
       "      <td>97.30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>94.4</td>\n",
       "      <td>18.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>72.9</td>\n",
       "      <td>73.42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>73.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>85.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>98.9</td>\n",
       "      <td>17.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>68.6</td>\n",
       "      <td>75.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>94.1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>103.32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>62.3</td>\n",
       "      <td>12.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>82.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42.5</td>\n",
       "      <td>10.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.8</td>\n",
       "      <td>129.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.4</td>\n",
       "      <td>15.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>89.8</td>\n",
       "      <td>105.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>121.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.5</td>\n",
       "      <td>112.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>71.2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>110.40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>89.8</td>\n",
       "      <td>96.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>106.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>133.23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>18.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>73.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59.8</td>\n",
       "      <td>16.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>97.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>74.2</td>\n",
       "      <td>18.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>78.3</td>\n",
       "      <td>80.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>82.31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>78.5</td>\n",
       "      <td>111.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>238.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>29.3</td>\n",
       "      <td>62.6</td>\n",
       "      <td>85.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50.2</td>\n",
       "      <td>39.7</td>\n",
       "      <td>147.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>34.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>49.2</td>\n",
       "      <td>114.28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>46.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.1</td>\n",
       "      <td>83.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>270.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>106.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>55.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>35.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>57.8</td>\n",
       "      <td>112.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>84.2</td>\n",
       "      <td>110.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>48.8</td>\n",
       "      <td>28.6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>97.42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>99.7</td>\n",
       "      <td>21.6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>55.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>90.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>73.4</td>\n",
       "      <td>73.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>66.1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>109.08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.8</td>\n",
       "      <td>124.05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>27.0</td>\n",
       "      <td>46.6</td>\n",
       "      <td>137.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MinorityPop  FireReport  CrimeRate  HouseAge  Income  intercept  Flood_2  \\\n",
       "32          1.5         5.0       32.0      61.8  118.76          1        1   \n",
       "12          1.0         2.2        9.0      44.4  127.65          1        0   \n",
       "6           1.1         2.2       14.0      40.2  137.22          1        0   \n",
       "24         19.6        10.5       36.0      73.5   99.48          1        1   \n",
       "11          1.8         5.4       27.0      85.1  116.00          1        0   \n",
       "22         10.0         6.2       29.0      60.4  117.44          1        1   \n",
       "7           1.0         5.7       11.0      27.9  162.50          1        0   \n",
       "8           1.7         2.0       11.0       7.7  136.86          1        1   \n",
       "5          43.1        29.1       34.0      82.7   79.95          1        0   \n",
       "1           4.9        11.0       75.0      42.6  214.80          1        1   \n",
       "26         24.5         8.6       53.0      81.4   97.30          1        1   \n",
       "16         94.4        18.4       32.0      72.9   73.42          1        0   \n",
       "30         73.5         9.0       39.0      75.4   85.64          1        0   \n",
       "34         98.9        17.4       32.0      68.6   75.20          1        0   \n",
       "38         94.1        10.5       42.0      55.9  103.32          1        1   \n",
       "21         62.3        12.2       46.0      48.0   82.12          1        1   \n",
       "42         42.5        10.4       25.0      40.8  129.60          1        0   \n",
       "14         13.4        15.1       30.0      89.8  105.10          1        1   \n",
       "31         10.7         3.6       15.0      20.8  121.02          1        0   \n",
       "27          4.4         5.6       23.0      71.5  112.30          1        0   \n",
       "37         71.2        11.9       46.0      57.0  110.40          1        1   \n",
       "4          21.5        15.1       25.0      89.8   96.31          1        0   \n",
       "25         17.3         7.7       37.0      66.9  106.56          1        0   \n",
       "41          1.0         4.8       19.0      15.2  133.23          1        1   \n",
       "49         18.0         7.3       31.2      18.2   73.50          1        0   \n",
       "15         59.8        16.5       40.0      72.7   97.84          1        0   \n",
       "19         74.2        18.5       22.0      78.3   80.14          1        0   \n",
       "0          54.0        34.1       68.0      52.6   82.31          1        1   \n",
       "2           7.1         6.9       18.0      78.5  111.04          1        0   \n",
       "36          1.4         3.4       17.0       2.0  238.42          1        1   \n",
       "48         48.2         3.6       29.3      62.6   85.20          1        0   \n",
       "18         50.2        39.7      147.0      83.0   74.59          1        0   \n",
       "45         34.0         7.1       23.0      49.2  114.28          1        1   \n",
       "28         46.2        21.8        4.0      73.1   83.30          1        0   \n",
       "47         23.7         1.5       18.0      22.0  270.20          1        1   \n",
       "3           5.3         7.3       31.0      90.1  106.94          1        0   \n",
       "20         55.5        23.3       29.0      79.0   81.77          1        0   \n",
       "43         35.1        15.6       28.0      57.8  112.60          1        0   \n",
       "13          2.5         7.2       29.0      84.2  110.84          1        0   \n",
       "33         48.8        28.6       27.0      78.1   97.42          1        0   \n",
       "29         99.7        21.6       31.0      65.0   55.83          1        0   \n",
       "35         90.6        11.3       34.0      73.4   73.88          1        0   \n",
       "39         66.1        10.7       43.0      67.5  109.08          1        0   \n",
       "9           1.6         2.5       22.0      63.8  124.05          1        0   \n",
       "46          3.1         4.9       27.0      46.6  137.31          1        0   \n",
       "\n",
       "    Flood_3  \n",
       "32        0  \n",
       "12        1  \n",
       "6         0  \n",
       "24        0  \n",
       "11        1  \n",
       "22        0  \n",
       "7         1  \n",
       "8         0  \n",
       "5         0  \n",
       "1         0  \n",
       "26        0  \n",
       "16        0  \n",
       "30        0  \n",
       "34        0  \n",
       "38        0  \n",
       "21        0  \n",
       "42        1  \n",
       "14        0  \n",
       "31        1  \n",
       "27        1  \n",
       "37        0  \n",
       "4         0  \n",
       "25        1  \n",
       "41        0  \n",
       "49        0  \n",
       "15        0  \n",
       "19        1  \n",
       "0         0  \n",
       "2         0  \n",
       "36        0  \n",
       "48        1  \n",
       "18        0  \n",
       "45        0  \n",
       "28        0  \n",
       "47        0  \n",
       "3         1  \n",
       "20        0  \n",
       "43        1  \n",
       "13        1  \n",
       "33        0  \n",
       "29        0  \n",
       "35        0  \n",
       "39        1  \n",
       "9         0  \n",
       "46        0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01054543])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_dpt = [1,7,34,90,1000,1,1,0]\n",
    "ols.predict(future_dpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024058556819344275"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_test_pred = ols.predict(X_test)\n",
    "mean_absolute_error(y_test,ols_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {'Model':[],'MeanAbsoluteError':[],'0-1TargetRangeSatisfied':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('OrdinaryLeastSquares')\n",
    "result_dict['MeanAbsoluteError'].append(0.0009937013883552562)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>Declination</td>   <th>  No. Observations:  </th>   <td>    45</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>   <td>    37</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Gaussian</td>     <th>  Df Model:          </th>   <td>     7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>       <td>identity</td>     <th>  Scale:             </th> <td>1.4828e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th>  <td>  190.73</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 01 Dec 2021</td> <th>  Deviance:          </th> <td>0.00054863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:13:45</td>     <th>  Pearson chi2:      </th>  <td>0.000549</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>3</td>        <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MinorityPop</th> <td> 7.382e-05</td> <td> 2.34e-05</td> <td>    3.155</td> <td> 0.002</td> <td>  2.8e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FireReport</th>  <td>    0.0003</td> <td>    0.000</td> <td>    3.219</td> <td> 0.001</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CrimeRate</th>   <td>-8.657e-05</td> <td> 3.36e-05</td> <td>   -2.573</td> <td> 0.010</td> <td>   -0.000</td> <td>-2.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HouseAge</th>    <td> 3.345e-05</td> <td> 3.29e-05</td> <td>    1.016</td> <td> 0.310</td> <td>-3.11e-05</td> <td>  9.8e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Income</th>      <td>-1.531e-05</td> <td> 2.05e-05</td> <td>   -0.745</td> <td> 0.456</td> <td>-5.56e-05</td> <td> 2.49e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>   <td>    0.0028</td> <td>    0.004</td> <td>    0.693</td> <td> 0.488</td> <td>   -0.005</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Flood_2</th>     <td>   -0.0005</td> <td>    0.002</td> <td>   -0.298</td> <td> 0.766</td> <td>   -0.004</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Flood_3</th>     <td>   -0.0007</td> <td>    0.002</td> <td>   -0.422</td> <td> 0.673</td> <td>   -0.004</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:            Declination   No. Observations:                   45\n",
       "Model:                            GLM   Df Residuals:                       37\n",
       "Model Family:                Gaussian   Df Model:                            7\n",
       "Link Function:               identity   Scale:                      1.4828e-05\n",
       "Method:                          IRLS   Log-Likelihood:                 190.73\n",
       "Date:                Wed, 01 Dec 2021   Deviance:                   0.00054863\n",
       "Time:                        11:13:45   Pearson chi2:                 0.000549\n",
       "No. Iterations:                     3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "MinorityPop  7.382e-05   2.34e-05      3.155      0.002     2.8e-05       0.000\n",
       "FireReport      0.0003      0.000      3.219      0.001       0.000       0.001\n",
       "CrimeRate   -8.657e-05   3.36e-05     -2.573      0.010      -0.000   -2.06e-05\n",
       "HouseAge     3.345e-05   3.29e-05      1.016      0.310   -3.11e-05     9.8e-05\n",
       "Income      -1.531e-05   2.05e-05     -0.745      0.456   -5.56e-05    2.49e-05\n",
       "intercept       0.0028      0.004      0.693      0.488      -0.005       0.011\n",
       "Flood_2        -0.0005      0.002     -0.298      0.766      -0.004       0.003\n",
       "Flood_3        -0.0007      0.002     -0.422      0.673      -0.004       0.002\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_reg = sm.GLM(y_train,X_train).fit()\n",
    "glm_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026340370085357156"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_reg_pred = glm_reg.predict(X_train)\n",
    "mean_absolute_error(y_train,glm_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6673153984864949"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train,glm_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48    0.004560\n",
       "15    0.010461\n",
       "30    0.009411\n",
       "4     0.009249\n",
       "22    0.002899\n",
       "47    0.000100\n",
       "5     0.014330\n",
       "1    -0.001484\n",
       "0     0.011668\n",
       "28    0.014265\n",
       "40    0.006698\n",
       "39    0.006932\n",
       "35    0.011808\n",
       "36   -0.001373\n",
       "24    0.005120\n",
       "14    0.007263\n",
       "8    -0.000027\n",
       "33    0.014781\n",
       "41    0.000510\n",
       "10    0.001819\n",
       "26    0.003846\n",
       "9     0.002688\n",
       "3     0.003467\n",
       "45    0.004949\n",
       "27    0.002662\n",
       "19    0.012452\n",
       "34    0.014268\n",
       "42    0.005247\n",
       "21    0.006980\n",
       "49    0.003159\n",
       "6     0.002057\n",
       "31    0.000855\n",
       "37    0.007632\n",
       "11    0.002637\n",
       "44    0.005349\n",
       "13    0.003103\n",
       "7     0.001034\n",
       "2     0.005566\n",
       "12    0.001165\n",
       "20    0.013629\n",
       "23    0.003860\n",
       "16    0.014463\n",
       "29    0.015769\n",
       "43    0.007014\n",
       "25    0.002963\n",
       "dtype: float64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024058556819344275"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_reg_pred_test = glm_reg.predict(X_test)\n",
    "mean_absolute_error(y_test,glm_reg_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8221457750954279"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,glm_reg_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('GeneralizedLinearModel - GaussianDist')\n",
    "result_dict['MeanAbsoluteError'].append(0.0009937013883552562)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels GLM with Binomial Family and Logit link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "glm = sm.GLM(y_train,X_train,family=sm.families.Binomial(link=sm.families.links.logit)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>Declination</td>   <th>  No. Observations:  </th>  <td>    45</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>    37</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -1.3392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 01 Dec 2021</td> <th>  Deviance:          </th> <td>0.097370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:14:53</td>     <th>  Pearson chi2:      </th>  <td>0.0942</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>9</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MinorityPop</th> <td>    0.0076</td> <td>    0.090</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.169</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FireReport</th>  <td>    0.0338</td> <td>    0.298</td> <td>    0.113</td> <td> 0.910</td> <td>   -0.551</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CrimeRate</th>   <td>   -0.0096</td> <td>    0.097</td> <td>   -0.099</td> <td> 0.921</td> <td>   -0.200</td> <td>    0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HouseAge</th>    <td>    0.0119</td> <td>    0.147</td> <td>    0.081</td> <td> 0.935</td> <td>   -0.277</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Income</th>      <td>   -0.0168</td> <td>    0.163</td> <td>   -0.103</td> <td> 0.918</td> <td>   -0.335</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>   <td>   -4.7883</td> <td>   24.039</td> <td>   -0.199</td> <td> 0.842</td> <td>  -51.904</td> <td>   42.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Flood_2</th>     <td>    0.1796</td> <td>    5.792</td> <td>    0.031</td> <td> 0.975</td> <td>  -11.173</td> <td>   11.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Flood_3</th>     <td>    0.1589</td> <td>    5.566</td> <td>    0.029</td> <td> 0.977</td> <td>  -10.750</td> <td>   11.068</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:            Declination   No. Observations:                   45\n",
       "Model:                            GLM   Df Residuals:                       37\n",
       "Model Family:                Binomial   Df Model:                            7\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -1.3392\n",
       "Date:                Wed, 01 Dec 2021   Deviance:                     0.097370\n",
       "Time:                        11:14:53   Pearson chi2:                   0.0942\n",
       "No. Iterations:                     9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "MinorityPop     0.0076      0.090      0.084      0.933      -0.169       0.184\n",
       "FireReport      0.0338      0.298      0.113      0.910      -0.551       0.619\n",
       "CrimeRate      -0.0096      0.097     -0.099      0.921      -0.200       0.181\n",
       "HouseAge        0.0119      0.147      0.081      0.935      -0.277       0.301\n",
       "Income         -0.0168      0.163     -0.103      0.918      -0.335       0.302\n",
       "intercept      -4.7883     24.039     -0.199      0.842     -51.904      42.328\n",
       "Flood_2         0.1796      5.792      0.031      0.975     -11.173      11.532\n",
       "Flood_3         0.1589      5.566      0.029      0.977     -10.750      11.068\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pred = glm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026176820908033497"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_train,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195946270707994"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006217777777777781"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 0.019\n",
      "0.0002484469898937148 0.021212203447865185\n"
     ]
    }
   ],
   "source": [
    "print(min(y_train), max(y_train))\n",
    "print(min(train_pred),max(train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005006170833671414"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_logit = glm.predict(X_test)\n",
    "mean_absolute_error(y_test,test_pred_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17550435908543338"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,test_pred_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('GeneralizedLinearModel - Binomial&Logitlink')\n",
    "result_dict['MeanAbsoluteError'].append(0.003427252359408609)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Binomial Families and Logit link - Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurumanikanta/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "glm_std = sm.GLM(y_scaled_train, X_scaled_train, family=sm.families.Binomial(sm.families.links.logit)).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027111005137465663"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_std_log = glm_std.predict(X_scaled_train)\n",
    "mean_absolute_error(y_scaled_train,train_pred_std_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029190020219519576"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_std_log = glm_std.predict(X_scaled_test)\n",
    "mean_absolute_error(y_scaled_test,test_pred_std_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6158694057602296"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_scaled_test,test_pred_std_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.005528879529053601\n",
      "0.013999999999999999 0.011809040812865193\n",
      "0.009000000000000001 0.005557675091545883\n",
      "0.0002 0.004779971201060598\n",
      "0.0002 0.0001701863924276954\n"
     ]
    }
   ],
   "source": [
    "for va1, va2 in zip(y_scaled_test, test_pred_std_log):\n",
    "    print(va1,va2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('GLM - Binomial&Logitlink StandardizedData')\n",
    "result_dict['MeanAbsoluteError'].append(0.002954389686655085)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Logit Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012606508064217894\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "class LogitRegression(LinearRegression):\n",
    "\n",
    "    def fit(self, x, p):\n",
    "        p = np.asarray(p)\n",
    "        y = np.log(p / (1 - p))\n",
    "        return super().fit(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y = super().predict(x)\n",
    "        return 1 / (np.exp(-y) + 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = len(X_train)\n",
    "noise = 0.00001 * np.random.randn(n)\n",
    "p = np.tanh(y_train + noise)\n",
    "\n",
    "model = LogitRegression()\n",
    "model.fit(X_train, p)\n",
    "\n",
    "print(mean_absolute_error(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021154350450528707 0.0002\n",
      "0.00018522930475486503 0.0002\n",
      "0.00018455056531464935 0.0002\n",
      "0.012000178563338607 0.012\n",
      "0.00018530204024349748 0.0002\n",
      "0.0001911847350311947 0.0002\n",
      "0.00020470438525241284 0.0002\n",
      "0.00019952152459984295 0.0002\n",
      "0.018999684129005382 0.019\n",
      "0.00019286942017430754 0.0002\n",
      "0.006999531707520339 0.006999999999999999\n",
      "0.018018326202678715 0.018000000000000002\n",
      "0.003981026343737885 0.004\n",
      "0.021995292265791867 0.022000000000000002\n",
      "0.009003081382329163 0.009000000000000001\n",
      "0.005996298941533346 0.006\n",
      "0.005004859078799007 0.005\n",
      "0.007985477128068586 0.008\n",
      "0.00018730072580328886 0.0002\n",
      "0.00299893476173135 0.003\n",
      "0.008978187337537953 0.009000000000000001\n",
      "0.011002123187085131 0.011000000000000001\n",
      "0.005002557687548497 0.005\n",
      "0.00018349275640093226 0.0002\n",
      "0.012017181117635065 0.012\n",
      "0.007986977490088734 0.008\n",
      "0.018986298458558763 0.019\n",
      "0.002996532032644977 0.003\n",
      "0.0001973501750560764 0.0002\n",
      "0.00019489830551346928 0.0002\n",
      "0.007491909721132138 0.0075\n",
      "0.0089955159737005 0.009000000000000001\n",
      "0.0029907773120520795 0.003\n",
      "0.01300072912985688 0.013000000000000001\n",
      "0.0006981669707084613 0.0007000000000000001\n",
      "0.0039825548323600595 0.004\n",
      "0.014986528165552987 0.015\n",
      "0.0100169471684766 0.01\n",
      "0.00199570236172429 0.002\n",
      "0.014004967153059393 0.013999999999999999\n",
      "0.009008963973629555 0.009000000000000001\n",
      "0.007998613213972024 0.008\n",
      "0.003984992008153856 0.004\n",
      "0.00019667200765703546 0.0002\n",
      "0.0001980552650575087 0.0002\n"
     ]
    }
   ],
   "source": [
    "for v1, v2 in zip(p,y_train):\n",
    "    print(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003193732433132612"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train,model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0031816587899950012"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n = len(X_train)\n",
    "#noise = 0.00001 * np.random.randn(n)\n",
    "p = np.tanh(y_train)\n",
    "\n",
    "model = LogitRegression()\n",
    "model.fit(X_train, p)\n",
    "\n",
    "mean_absolute_error(y_train,model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.638760463149394"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 0.0001999999973333334\n",
      "0.0002 0.0001999999973333334\n",
      "0.005 0.004999958333749996\n",
      "0.013000000000000001 0.012999267716169015\n",
      "0.009000000000000001 0.008999757007872942\n",
      "0.009000000000000001 0.008999757007872942\n",
      "0.019 0.018997713996764965\n",
      "0.01 0.00999966667999946\n",
      "0.019 0.018997713996764965\n",
      "0.004 0.003999978666803199\n",
      "0.011000000000000001 0.01099955635480575\n",
      "0.003 0.0029999910000324\n",
      "0.0002 0.0001999999973333334\n",
      "0.009000000000000001 0.008999757007872942\n",
      "0.0002 0.0001999999973333334\n",
      "0.0002 0.0001999999973333334\n",
      "0.0002 0.0001999999973333334\n",
      "0.004 0.003999978666803199\n",
      "0.0007000000000000001 0.0006999998856666892\n",
      "0.0002 0.0001999999973333334\n",
      "0.0002 0.0001999999973333334\n",
      "0.018000000000000002 0.017998056251909367\n",
      "0.009000000000000001 0.008999757007872942\n",
      "0.008 0.007999829337702286\n",
      "0.003 0.0029999910000324\n",
      "0.001 0.0009999996666668\n",
      "0.0002 0.0001999999973333334\n",
      "0.0002 0.0001999999973333334\n",
      "0.005 0.004999958333749996\n",
      "0.013999999999999999 0.01399908540503751\n",
      "0.0002 0.0001999999973333334\n",
      "0.0002 0.0001999999973333334\n",
      "0.003 0.0029999910000324\n",
      "0.0002 0.0001999999973333334\n",
      "0.006999999999999999 0.006999885668907555\n",
      "0.0075 0.00749985937816399\n",
      "0.009000000000000001 0.008999757007872942\n",
      "0.008 0.007999829337702286\n",
      "0.004 0.003999978666803199\n",
      "0.012 0.011999424033175667\n",
      "0.012 0.011999424033175667\n",
      "0.015 0.01499887510124078\n",
      "0.002 0.0019999973333376\n",
      "0.018000000000000002 0.017998056251909367\n",
      "0.006 0.005999928001036785\n"
     ]
    }
   ],
   "source": [
    "for v1, v2 in zip(y_train,p):\n",
    "    print(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('LinearRegression-tanh&logit')\n",
    "result_dict['MeanAbsoluteError'].append(0.011502672724507248)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liner Regression with Logit Transformation - Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogitRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(X_train)\n",
    "noise = 0.00001 * np.random.randn(n)\n",
    "p = np.tanh(y_train + noise)\n",
    "\n",
    "lr_std = LogitRegression()\n",
    "lr_std.fit(X_scaled_train, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037005368041537834"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_std_lr = lr_std.predict(X_scaled_train)\n",
    "mean_absolute_error(y_train,train_pred_std_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002904200084980783"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_std_lr = lr_std.predict(X_scaled_test)\n",
    "mean_absolute_error(y_test,test_pred_std_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019 0.010230266091882639\n",
      "0.015 0.010270293033004077\n",
      "0.0002 0.00046192286632792213\n",
      "0.0002 0.00036642184938967943\n",
      "0.0007000000000000001 0.00010678516592697031\n"
     ]
    }
   ],
   "source": [
    "for va1, va2 in zip(y_test, test_pred_std_lr):\n",
    "    print(va1,va2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('LinearRegression-tanh&logit StandardizedData')\n",
    "result_dict['MeanAbsoluteError'].append(0.002904200084980783)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000936688354784383"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(X_train,y_train)\n",
    "predictions = rfr.predict(X_train)\n",
    "mean_absolute_error(y_train,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00029089, 0.00028134, 0.00022534, 0.01043083, 0.00050032,\n",
       "       0.0004228 , 0.00029934, 0.00022534, 0.01685   , 0.00171392,\n",
       "       0.00731708, 0.01677   , 0.00561875, 0.0176155 , 0.00848583,\n",
       "       0.0059195 , 0.00457094, 0.00790975, 0.00036534, 0.00247963,\n",
       "       0.00721178, 0.01065042, 0.00552146, 0.00022534, 0.01054533,\n",
       "       0.0085425 , 0.01737   , 0.00664   , 0.00159711, 0.00022534,\n",
       "       0.00763284, 0.01095   , 0.00386605, 0.01295   , 0.00183101,\n",
       "       0.0045665 , 0.0140225 , 0.00796644, 0.00237593, 0.0137    ,\n",
       "       0.012495  , 0.01005242, 0.00532125, 0.00022534, 0.00027089])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004253504514920925"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_rfr = rfr.predict(X_test)\n",
    "mean_absolute_error(y_test,test_pred_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.465596410886593"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,test_pred_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('RandomForestRegressors')\n",
    "result_dict['MeanAbsoluteError'].append(0.003835015896023483)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests -  Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr_std = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019996600529100543"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_std.fit(X_scaled_train,y_train)\n",
    "predictions = rfr_std.predict(X_scaled_train)\n",
    "mean_absolute_error(y_train,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00164459, 0.00421367, 0.00242287, 0.00877133, 0.00419673,\n",
       "       0.001332  , 0.00094781, 0.00208874, 0.014142  , 0.00127214,\n",
       "       0.00551191, 0.013253  , 0.005546  , 0.01511863, 0.0088845 ,\n",
       "       0.00684856, 0.00559292, 0.00830717, 0.001538  , 0.005203  ,\n",
       "       0.009892  , 0.00864711, 0.00376525, 0.00438908, 0.01046467,\n",
       "       0.00494   , 0.01370925, 0.00272473, 0.00283223, 0.00130536,\n",
       "       0.0071472 , 0.00884174, 0.00359429, 0.01058381, 0.00463314,\n",
       "       0.00527463, 0.01254736, 0.00925217, 0.001664  , 0.011008  ,\n",
       "       0.00985728, 0.00637211, 0.00675722, 0.00135   , 0.00129406])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007173434632034631"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_rfr_std = rfr_std.predict(X_scaled_test)\n",
    "mean_absolute_error(y_test,test_pred_rfr_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4058719654991023"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_scaled_test,test_pred_rfr_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019 0.013032777777777777\n",
      "0.015 0.011037777777777782\n",
      "0.0002 0.00027733333333333283\n",
      "0.0002 0.00020133333333333323\n",
      "0.0007000000000000001 0.0032650500000000015\n"
     ]
    }
   ],
   "source": [
    "for v1, v2 in zip(y_test,test_pred_rfr_std):\n",
    "    print(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('RandomForestRegressor-StandardizedData')\n",
    "result_dict['MeanAbsoluteError'].append(0.0025146322222222217)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MinorityPop', 'FireReport', 'CrimeRate', 'HouseAge', 'Income',\n",
       "       'intercept', 'Flood_2', 'Flood_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00500832, 0.02142036, 0.24484393, 0.42137973,\n",
       "       0.06843183, 0.03390637, 0.20500946])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_std.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1623931623931644e-05"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train,y_train)\n",
    "predictions = dt.predict(X_train)\n",
    "mean_absolute_error(y_train,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002, 0.0002, 0.005 , 0.013 , 0.009 , 0.009 , 0.019 , 0.01  ,\n",
       "       0.019 , 0.004 , 0.011 , 0.003 , 0.0002, 0.009 , 0.0002, 0.0002,\n",
       "       0.0002, 0.004 , 0.0007, 0.0002, 0.0002, 0.018 , 0.009 , 0.008 ,\n",
       "       0.003 , 0.001 , 0.0002, 0.0002, 0.005 , 0.014 , 0.0002, 0.0002,\n",
       "       0.003 , 0.0002, 0.007 , 0.0075, 0.009 , 0.008 , 0.004 , 0.012 ,\n",
       "       0.012 , 0.015 , 0.002 , 0.018 , 0.006 ])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0051576923076923084"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_dt = dt.predict(X_test)\n",
    "mean_absolute_error(y_test,test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18700367571369203"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MinorityPop', 'FireReport', 'CrimeRate', 'HouseAge', 'Income',\n",
       "       'intercept', 'Flood_2', 'Flood_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22872267, 0.52236922, 0.09857405, 0.01160782, 0.10664416,\n",
       "       0.        , 0.        , 0.03208207])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('DecisionTree')\n",
    "result_dict['MeanAbsoluteError'].append(0.0036000000000000003)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees - Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_std = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4814814814814817e-05"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_std.fit(X_scaled_train,y_train)\n",
    "predictions_dt_std = dt_std.predict(X_scaled_train)\n",
    "mean_absolute_error(y_train,predictions_dt_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002    , 0.0002    , 0.0002    , 0.012     , 0.00036667,\n",
       "       0.0002    , 0.0002    , 0.0002    , 0.019     , 0.0002    ,\n",
       "       0.007     , 0.018     , 0.004     , 0.022     , 0.009     ,\n",
       "       0.006     , 0.005     , 0.008     , 0.0002    , 0.003     ,\n",
       "       0.009     , 0.011     , 0.005     , 0.0002    , 0.012     ,\n",
       "       0.008     , 0.019     , 0.003     , 0.00036667, 0.0002    ,\n",
       "       0.0075    , 0.009     , 0.003     , 0.013     , 0.00036667,\n",
       "       0.004     , 0.015     , 0.01      , 0.002     , 0.014     ,\n",
       "       0.009     , 0.008     , 0.004     , 0.0002    , 0.0002    ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dt_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0045200000000000014"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_dt_std = dt.predict(X_scaled_test)\n",
    "mean_absolute_error(y_scaled_test,test_pred_dt_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11048040152963656"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_scaled_test,test_pred_dt_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019 0.013999999999999999\n",
      "0.015 0.00020000000000000006\n",
      "0.0002 0.00020000000000000006\n",
      "0.0002 0.004\n",
      "0.0007000000000000001 0.012\n"
     ]
    }
   ],
   "source": [
    "for v1, v2 in zip(y_test,predictions_dt_std):\n",
    "    print(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['Model'].append('DecisionTree-StandardizedData')\n",
    "result_dict['MeanAbsoluteError'].append(0.006980000000000002)\n",
    "result_dict['0-1TargetRangeSatisfied'].append('Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MeanAbsoluteError</th>\n",
       "      <th>0-1TargetRangeSatisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OrdinaryLeastSquares</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GeneralizedLinearModel - GaussianDist</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GeneralizedLinearModel - Binomial&amp;Logitlink</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLM - Binomial&amp;Logitlink StandardizedData</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearRegression-tanh&amp;logit</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearRegression-tanh&amp;logit StandardizedData</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestRegressors</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestRegressor-StandardizedData</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTree-StandardizedData</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model  MeanAbsoluteError  \\\n",
       "0                          OrdinaryLeastSquares           0.000994   \n",
       "1         GeneralizedLinearModel - GaussianDist           0.000994   \n",
       "2   GeneralizedLinearModel - Binomial&Logitlink           0.003427   \n",
       "3     GLM - Binomial&Logitlink StandardizedData           0.002954   \n",
       "4                   LinearRegression-tanh&logit           0.011503   \n",
       "5  LinearRegression-tanh&logit StandardizedData           0.002904   \n",
       "6                        RandomForestRegressors           0.003835   \n",
       "7        RandomForestRegressor-StandardizedData           0.002515   \n",
       "8                                  DecisionTree           0.003600   \n",
       "9                 DecisionTree-StandardizedData           0.006980   \n",
       "\n",
       "  0-1TargetRangeSatisfied  \n",
       "0                      No  \n",
       "1                      No  \n",
       "2                     Yes  \n",
       "3                     Yes  \n",
       "4                     Yes  \n",
       "5                     Yes  \n",
       "6                     Yes  \n",
       "7                     Yes  \n",
       "8                     Yes  \n",
       "9                     Yes  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(result_dict)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='sigmoid',degree=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006684444444444447"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.fit(X_train,y_train)\n",
    "predictions = svr.predict(X_train)\n",
    "mean_absolute_error(y_train,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
       "       0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
       "       0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
       "       0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
       "       0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
       "       0.0111, 0.0111, 0.0111, 0.0111, 0.0111])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Distribution transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import loggamma\n",
    "from scipy.special import expit, logit\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "def logLikelihood(params, y, X):\n",
    "    b = np.array(params[0:-1])      # the beta parameters of the regression model\n",
    "    phi = params[-1]                # the phi parameter\n",
    "    mu = expit(np.dot(X,b))\n",
    "   \n",
    "    eps = 1e-6                      # used for safety of the gamma and log functions avoiding inf\n",
    "    res = - np.sum(loggamma(phi+eps) # the log likelihood\n",
    "                   - loggamma(mu*phi+eps) \n",
    "                   - loggamma((1-mu)*phi+eps) \n",
    "                   + (mu*phi-1)*np.log(y+eps) \n",
    "                   + ((1-mu)*phi-1)*np.log(1-y+eps))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initial parameters for optimization\n",
    "phi = 1\n",
    "b0 = 1\n",
    "x0 = np.array([b0,b0,b0,b0,b0,b0,b0,b0,phi])\n",
    "\n",
    "res = minimize(logLikelihood, x0=x0, args=(y_train,X_train), bounds=[(None,None), \n",
    "                                                         (None,None), \n",
    "                                                         (None,None), \n",
    "                                                         (None,None), \n",
    "                                                         (None,None), \n",
    "                                                        (None,None), \n",
    "                                                        (None,None), \n",
    "                                                        (None,None), \n",
    "                                                         (0,None)])\n",
    "\n",
    "b = np.array(res.x[0:X.shape[1]])   # optimal regression parameters\n",
    "y_ = expit(np.dot(X,b))             # predictions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8472978603872036 0.3\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-0.4054651081081643 0.4\n",
      "nan 1.1\n",
      "nan 1.9\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "-1.3862943611198906 0.2\n",
      "1.3862943611198908 0.8\n",
      "1.3862943611198908 0.8\n",
      "nan 1.8\n",
      "nan 1.8\n",
      "2.1972245773362196 0.9\n",
      "nan 1.9\n",
      "nan 1.5\n",
      "0.4054651081081642 0.6\n",
      "-3.8918202981106265 0.02\n",
      "-2.197224577336219 0.1\n",
      "nan 1.2\n",
      "0.0 0.5\n",
      "0.8472978603872034 0.7\n",
      "-0.8472978603872036 0.3\n",
      "nan 1.3\n",
      "2.1972245773362196 0.9\n",
      "-0.4054651081081643 0.4\n",
      "-3.8918202981106265 0.02\n",
      "-3.8918202981106265 0.02\n",
      "nan 1.4\n",
      "nan 2.2\n",
      "1.3862943611198908 0.8\n",
      "-3.8918202981106265 0.02\n",
      "2.1972245773362196 0.9\n",
      "2.1972245773362196 0.9\n",
      "-0.4054651081081643 0.4\n",
      "2.1972245773362196 0.9\n",
      "-3.8918202981106265 0.02\n",
      "0.0 0.5\n",
      "inf 1.0\n",
      "-1.3862943611198906 0.2\n",
      "-0.8472978603872036 0.3\n",
      "-3.8918202981106265 0.02\n",
      "-2.5866893440979424 0.07\n",
      "1.0986122886681098 0.75\n",
      "nan 1.2\n"
     ]
    }
   ],
   "source": [
    "for val1, val2 in zip(y_temp,houses['Declination']):\n",
    "    print(val1, val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
